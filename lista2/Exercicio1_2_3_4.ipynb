{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Qual técnica de regressão linear você usaria se tivesse um conjunto de treinamento com milhares de features? Explique por quais razões você utilizaria esta técnica.\n",
    "\n",
    "    R: Regressão linear com gradiente descendente estocástico. Pois ele ajusta os parâmetros de forma iterativa em relação aos testes durante o processo de treinamento, fazendo assim que a convergencia para um conjunto de dados enorme seja mais rápido.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suponha que as features (i.e., atributos) do seu conjunto de treinamento tenham escalas muito diferentes. Qual técnica de regressão linear pode sofrer com isso e como? O que pode ser feito para mitigar este problema?\n",
    "\n",
    "    R: Gradiente descendente. O que pode ser feito é utilizar de escalonamento de features utilizando padranização, pois ele permite termos variações similares em todos os atributos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suponha que você use o gradiente descendente em batelada e plote o erro de cada época. Se você perceber que o erro aumenta constantemente, o que provavelmente está acontecendo? Como você pode consertar isso?\n",
    "\n",
    "    R: Provavelmente a taxa de aprendizagem esteja muito alta, fazendo com que o erro fique saltando entre o minimo. Pode-se diminuir a taxa para que o sistema tenha uma mudança de parametro de forma mais suave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Entre os algoritmos baseados no gradiente descendente (GD) que discutimos (batch, estocástico e mini-batch), qual deles chega mais rapidamente à vizinhança da solução ótima? Qual deles realmente converge? O que você pode fazer para que os outros também convirjam?\n",
    "\n",
    "    R: O Gradiente Descendente Estocástico e Mini-batch. O Batch. Para o estocástico a solução é reduzir gradualmente a taxa de aprendizagem, para que ele consiga se livrar dos mínimos globais inicialmente (com grandes valores) e depois diminuindo os valores para estabilizar no mínimo global. Para o Mini-batch pode-se usar esquemas de variação do passo de aprendizagem para melhorar a convergência."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}